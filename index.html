<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Jeremy Zhengqi Huang ¬∑ Human-Centered AI & Accessibility</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="styles.css" />
  </head>

  <body>
    <!-- Desktop Navigation -->
    <nav id="navbar">
      <div class="nav-container">
        <ul class="nav-links">
          <li><a href="#about">About</a></li>
          <li><a href="#news">News</a></li>
          <li><a href="#publications">Publications</a></li>
          <li><a href="paper/Huang_CV_Latest.pdf">CV</a></li>
          <li><a href="#contact">Contact</a></li>
        </ul>
      </div>
    </nav>

    <!-- Mobile Navigation -->
    <div class="mobile-nav">
      <div class="mobile-nav-container">
        <a href="#" class="mobile-nav-name">Jeremy Zhengqi Huang</a>
        <button class="mobile-nav-toggle" id="mobile-menu-toggle">‚ò∞</button>
      </div>
      <ul class="mobile-nav-links" id="mobile-menu">
        <li><a href="#about">About</a></li>
        <li><a href="#news">News</a></li>
        <li><a href="#publications">Publications</a></li>
        <li><a href="paper/Huang_CV_Latest.pdf">CV</a></li>
        <li><a href="#contact">Contact</a></li>
      </ul>
    </div>

    <!-- Main Container with Two Columns -->
    <div class="main-container">
      <!-- Left Column - Sidebar -->
      <aside class="sidebar">
        <div class="profile-section">
          <img
            src="images/portrait.jpeg"
            alt="Portrait of Jeremy Zhengqi Huang"
            class="hero-image"
          />
          <h1>Jeremy Z. Huang</h1>
          <div class="title">Human-AI Interaction & Accessibility</div>
          <div class="institution">
            Computer Science & Engineering<br />University of Michigan
          </div>
        </div>

        <div class="sidebar-contact">
          <a href="mailto:zjhuang@umich.edu">Email</a>
          <a href="paper/Huang_CV.pdf">Curriculum Vitae</a>
          <a href="https://scholar.google.com/citations?user=tYjsMXYAAAAJ&hl=en"
            >Google Scholar</a
          >
          <a href="https://soundability.eecs.umich.edu/">Soundability Lab</a>
          <a href="https://github.com/zjhuang2">GitHub</a>
          <a href="https://twitter.com/jzh2o">X: @jzh2o</a>
        </div>

        <!-- <div class="sidebar-bio">
          <p>
            I design and study human-AI systems that help people with
            disabilities personalize sound and media so they can access
            information, communication, and everyday experiences on their own
            terms.
          </p>
          <p style="margin-top: 1rem">
            <strong style="color: var(--accent-color)">Advised by:</strong>
            <a href="https://web.eecs.umich.edu/~profdj/">Prof. Dhruv Jain</a>
          </p>
        </div> -->
      </aside>

      <!-- Right Column - Main Content -->
      <main class="content">
        <!-- About Section -->
        <section id="about">
          <p>
            Hi!üëã I am a second-year Ph.D. student in Computer Science &
            Engineering at the University of Michigan, where I work with
            <a href="https://web.eecs.umich.edu/~profdj/">Prof. Dhruv Jain</a>
            in the
            <a href="https://soundability.eecs.umich.edu/">Soundability Lab</a>.
          </p>
          <p>
            My research interests lie at the intersection of Human-Computer
            Interaction, applied AI and accessibility. I focus on developing
            human-AI systems that enable people with disabilities to curate and
            adapt multi-modal content from both real-world and media contexts,
            transforming it into accessible formats that align with their
            cognitive and sensory needs.
            <!-- My research develops human-AI systems that help people make sense of
            complex multimodal information across real-world and media contexts.
            I build adaptive interfaces that let users transform inputs (like
            audio and video) into flexible, accessible formats that match their
            cognitive and sensory needs. -->
          </p>
        </section>

        <!-- News Section -->
        <section id="news">
          <h2>Recent News</h2>
          <div class="news-list">
            <article class="news-item">
              <div class="news-date">Aug 12, 2025</div>
              <p>
                Our work on transforming non-speech captions with anchored
                generative models is headed to <strong>ASSETS 2025</strong> in
                Denver!
              </p>
            </article>
            <article class="news-item">
              <div class="news-date">Apr 28, 2025</div>
              <p>
                I will present <em>SoundWeaver</em> at
                <strong>CHI 2025</strong> in Yokohama, sharing how we support
                real-time sensemaking of auditory environments.
              </p>
            </article>
            <article class="news-item">
              <div class="news-date">Mar 14, 2024</div>
              <p>
                Our Human-AI Collaborative Sound Awareness (HACS) paper was
                accepted to <strong>CHI 2024</strong>!
              </p>
            </article>
            <article class="news-item">
              <div class="news-date">Mar 1, 2024</div>
              <p>
                Thrilled to continue my Ph.D. journey at Michigan CSE and keep
                building accessible technologies with the Soundability Lab.
              </p>
            </article>
            <article class="news-item">
              <div class="news-date">Jun 30, 2023</div>
              <p>
                Our field study of mobile sound recognition systems received an
                <strong>Honorable Mention</strong> at ASSETS 2023.
              </p>
            </article>
          </div>
        </section>

        <!-- Publications Section -->
        <section id="publications">
          <h2>Publications</h2>

          <div class="publication" id="captune">
            <div class="pub-thumbnail">
              <img
                src="images/captune.png"
                alt="Illustration of the CapTune caption personalisation interface"
              />
            </div>
            <div class="pub-content">
              <div class="pub-title">
                CapTune: Transforming Non-Speech Captions with Anchored
                Generative Models
              </div>
              <div class="pub-authors">
                <strong>Jeremy Zhengqi Huang</strong>, Calu√£ de Lacerda Pataca,
                Liang-Yuan Wu, Dhruv Jain
              </div>
              <div class="pub-venue">ASSETS 2025</div>
              <div class="pub-links">
                <a
                  href="https://arxiv.org/abs/2508.19971"
                  target="_blank"
                  rel="noopener"
                  >Preprint</a
                >

                <a
                  href="https://zjhuang2.github.io/captune/"
                  target="_blank"
                  rel="noopener"
                  >Project Page</a
                >

                <a
                  href="https://youtu.be/X1BTCv0ZpKA?si=UmSrgX0PMumsk9jI"
                  target="_blank"
                  rel="noopener"
                  >Video</a
                >
              </div>
            </div>
          </div>

          <div class="publication" id="soundweaver">
            <div class="pub-thumbnail">
              <img
                src="images/soundweaver.png"
                alt="Concept art for the SoundWeaver auditory sensemaking system"
              />
            </div>
            <div class="pub-content">
              <div class="pub-title">
                Weaving Sound Information to Support Real-time Sensemaking of
                Auditory Environments
              </div>
              <div class="pub-authors">
                <strong>Jeremy Zhengqi Huang</strong>, Jaylin Herskovitz,
                Liang-Yuan Wu, Cecily Morrison, Dhruv Jain
              </div>
              <div class="pub-venue">CHI 2025</div>
              <div class="pub-links">
                <a href="paper/chi25-soundweaver.pdf" target="_blank">PDF</a>
                <a
                  href="https://dl.acm.org/doi/full/10.1145/3706598.3714268"
                  target="_blank"
                  rel="noopener"
                  >ACM DL</a
                >
              </div>
            </div>
          </div>

          <div class="publication" id="masksound">
            <div class="pub-thumbnail">
              <img
                src="images/masksound.png"
                alt="Prototype of a sound masking interface supporting autistic listeners"
              />
            </div>
            <div class="pub-content">
              <div class="pub-title">
                Exploring Sound Masking Approaches to Support People with Autism
                in Managing Noise Sensitivity
              </div>
              <div class="pub-authors">
                Anna Y. Park, Andy Jin, <strong>Jeremy Zhengqi Huang</strong>,
                Jesse Carr, Dhruv Jain
              </div>
              <div class="pub-venue">ASSETS 2024</div>
              <div class="pub-links">
                <a
                  href="https://dl.acm.org/doi/10.1145/3663548.3675656"
                  target="_blank"
                  rel="noopener"
                  >ACM DL</a
                >
              </div>
            </div>
          </div>

          <div class="publication">
            <div class="pub-thumbnail">
              <img
                src="images/human-ai-collab.png"
                alt="Diagram describing the Human-AI Collaborative Sound Awareness workflow"
              />
            </div>
            <div class="pub-content">
              <div class="pub-title">
                A Human-AI Collaborative Approach for Designing Sound Awareness
                Systems
              </div>
              <div class="pub-authors">
                <strong>Jeremy Zhengqi Huang</strong>, Wren Wood, Hriday
                Chhabria, Dhruv Jain
              </div>
              <div class="pub-venue">CHI 2024</div>
              <div class="pub-links">
                <a href="paper/chi24_hacs.pdf" target="_blank">PDF</a>
                <a
                  href="https://dl.acm.org/doi/10.1145/3613904.3642062"
                  target="_blank"
                  rel="noopener"
                  >ACM DL</a
                >
              </div>
            </div>
          </div>

          <div class="publication" id="not-there-yet">
            <div class="pub-thumbnail">
              <img
                src="images/not-there-yet.jpeg"
                alt="Smartwatch showing a detected car honk notification"
              />
            </div>
            <div class="pub-content">
              <div class="pub-title">
                Not There Yet: Feasibility and Challenges of Mobile Sound
                Recognition Systems
              </div>
              <div class="pub-authors">
                <strong>Jeremy Zhengqi Huang</strong>, Hriday Chhabria, Dhruv
                Jain
              </div>
              <div class="pub-venue">
                ASSETS 2023 <span class="pub-award">Honorable Mention</span>
              </div>
              <div class="pub-links">
                <a href="paper/ASSETS23-SWFieldStudy.pdf" target="_blank"
                  >PDF</a
                >
                <a
                  href="https://dl.acm.org/doi/abs/10.1145/3597638.3608431"
                  target="_blank"
                  rel="noopener"
                  >ACM DL</a
                >
                <a
                  href="https://youtu.be/JL7xvsFC9P0"
                  target="_blank"
                  rel="noopener"
                  >Video</a
                >
              </div>
            </div>
          </div>

          <div class="publication" id="adaptivesound">
            <div class="pub-thumbnail">
              <img
                src="images/adaptivesound.png"
                alt="AdaptiveSound interface showing sound predictions and feedback controls"
              />
            </div>
            <div class="pub-content">
              <div class="pub-title">
                AdaptiveSound: An Interactive Feedback-Loop System to Improve
                Sound Recognition for Deaf and Hard of Hearing Users
              </div>
              <div class="pub-authors">
                Hang Do, Quan Dang, <strong>Jeremy Zhengqi Huang</strong>, Dhruv
                Jain
              </div>
              <div class="pub-venue">ASSETS 2023</div>
              <div class="pub-links">
                <a href="paper/ASSETS23-AdaptiveSound.pdf" target="_blank"
                  >PDF</a
                >
                <a
                  href="https://dl.acm.org/doi/abs/10.1145/3597638.3608390"
                  target="_blank"
                  rel="noopener"
                  >ACM DL</a
                >
                <a
                  href="https://youtu.be/JL7xvsFC9P0"
                  target="_blank"
                  rel="noopener"
                  >Video</a
                >
              </div>
            </div>
          </div>
        </section>

        <!-- Projects Section -->
        <!-- <section id="projects">
          <h2>Current Projects</h2>

          <div class="project">
            <h3 class="project-title">SoundWeaver</h3>
            <p class="project-description">
              Designing a human-AI collaboration that weaves captions, sound
              effects, and contextual cues to help Deaf and Hard of Hearing
              people interpret rich auditory environments in real time. The
              project is headed to CHI 2025 and was co-designed with DHH
              partners over multiple iterations.
            </p>
          </div>

          <div class="project">
            <h3 class="project-title">CapTune</h3>
            <p class="project-description">
              Investigating anchored generative models that transform non-speech
              captions so they convey emotion, intensity, and narrative meaning.
              CapTune personalizes caption styles while keeping Deaf and hard of
              hearing viewers in control.
            </p>
          </div>

          <div class="project">
            <h3 class="project-title">
              Human-AI Collaborative Sound Awareness
            </h3>
            <p class="project-description">
              Building participatory workflows where Deaf and Hard of Hearing
              co-designers shape sound recognition systems alongside machine
              learning models. This work produced practical guidance for
              aligning algorithmic feedback with lived experience.
            </p>
          </div>

          <div class="project">
            <h3 class="project-title">AdaptiveSound</h3>
            <p class="project-description">
              Creating feedback loops that let people quickly correct and refine
              mobile sound recognition alerts. AdaptiveSound captures context,
              confidence, and personalization preferences to continually improve
              assistive notifications.
            </p>
          </div>
        </section> -->

        <!-- Events Section -->
        <!-- <section id="events">
          <h2>Upcoming & Recent Events</h2>

          <div class="events-list">
            <article class="event">
              <div class="event-date">Oct 25, 2025</div>
              <a
                href="https://assets25.sigaccess.org/"
                target="_blank"
                rel="noopener"
                >ASSETS 2025</a
              >
              <p>Presenting work with the SIGACCESS community in Denver.</p>
            </article>
            <article class="event">
              <div class="event-date">Apr 26, 2025</div>
              <a href="https://chi2024.acm.org" target="_blank" rel="noopener"
                >CHI 2025</a
              >
              <p>Sharing SoundWeaver research in Yokohama, Japan.</p>
            </article>
            <article class="event">
              <div class="event-date">Mar 28, 2025</div>
              <a href="https://chi2024.acm.org" target="_blank" rel="noopener"
                >DisVis 2025</a
              >
              <p>
                Participating in discussions on disability visibility in HCI.
              </p>
            </article>
            <article class="event">
              <div class="event-date">Oct 28, 2024</div>
              <a
                href="https://assets24.sigaccess.org/"
                target="_blank"
                rel="noopener"
                >ASSETS 2024</a
              >
              <p>
                Exploring sound masking approaches with accessibility
                researchers.
              </p>
            </article>
            <article class="event">
              <div class="event-date">May 11, 2024</div>
              <a href="https://chi2024.acm.org" target="_blank" rel="noopener"
                >CHI 2024</a
              >
              <p>Presenting Human-AI Collaborative Sound Awareness (HACS).</p>
            </article>
          </div>
        </section> -->

        <!-- CV Section -->
        <!-- <section id="cv">
          <h2>Curriculum Vitae</h2>

          <p>
            Download my full CV for complete publication, teaching, and service
            history:
            <a href="paper/Huang_CV_Mar17.pdf" target="_blank"
              >Huang_CV_Mar17.pdf</a
            >
          </p>

          <h3 style="margin-top: 2rem">Education</h3>
          <ul class="styled-list">
            <li>
              <strong>Ph.D. in Computer Science & Engineering</strong>,
              University of Michigan (2023&nbsp;‚Äì&nbsp;present)<br />
              <span style="color: var(--text-light); font-size: 0.95rem"
                >Advisor: Prof. Dhruv Jain ¬∑ Soundability Lab</span
              >
            </li>
            <li>
              <strong>M.S. in Human-Computer Interaction</strong>, University of
              Michigan School of Information, 2023
            </li>
            <li>
              <strong>B.S. in Data Science & Psychology</strong>, The Ohio State
              University
            </li>
          </ul>

          <h3 style="margin-top: 2rem">Selected Honors & Awards</h3>
          <ul class="styled-list">
            <li>
              <strong>Honorable Mention Paper Award</strong>, ASSETS 2023 for
              ‚ÄúNot There Yet: Feasibility and Challenges of Mobile Sound
              Recognition Systems‚Äù
            </li>
            <li>
              <strong>CHI 2025</strong> full paper on SoundWeaver (upcoming)
            </li>
            <li>
              <strong>CHI 2024</strong> full paper on Human-AI Collaborative
              Sound Awareness
            </li>
          </ul>

          <h3 style="margin-top: 2rem">Community & Service</h3>
          <ul class="styled-list">
            <li>
              Collaborating with the
              <a
                href="https://soundability.eecs.umich.edu/"
                target="_blank"
                rel="noopener"
                >Soundability Lab</a
              >
              on accessible sound technologies
            </li>
            <li>
              Active member of ACM SIGCHI and SIGACCESS communities, presenting
              at CHI and ASSETS
            </li>
          </ul>
        </section> -->

        <!-- Contact Section -->
        <section id="contact">
          <h2>Contact</h2>
          <p style="font-size: 1.05rem; margin-bottom: 2rem">
            I love connecting with researchers, designers, and community
            partners who care about creating a more accessible world. Feel free
            to reach out!
          </p>
          <div class="contact-grid">
            <div>
              <h3
                style="
                  color: var(--accent-color);
                  font-size: 1rem;
                  margin-bottom: 1rem;
                "
              >
                Connect
              </h3>
              <p style="margin-bottom: 0.5rem">
                <strong>Email:</strong>
                <a href="mailto:zjhuang@umich.edu">zjhuang@umich.edu</a>
              </p>
              <p style="margin-bottom: 0.5rem">
                <strong>GitHub:</strong>
                <a href="https://github.com/zjhuang2">@zjhuang2</a>
              </p>
              <p style="margin-bottom: 0.5rem">
                <strong>Scholar:</strong>
                <a
                  href="https://scholar.google.com/citations?user=tYjsMXYAAAAJ&hl=en"
                  >Google Scholar</a
                >
              </p>
              <p>
                <strong>Twitter/X:</strong>
                <a href="https://twitter.com/jzh2o">@jzh2o</a>
              </p>
            </div>
            <div>
              <h3
                style="
                  color: var(--accent-color);
                  font-size: 1rem;
                  margin-bottom: 1rem;
                "
              >
                Find Me
              </h3>
              <p style="margin-bottom: 0.5rem">
                Computer Science & Engineering<br />University of Michigan
              </p>
              <p style="margin-bottom: 0.5rem">
                Soundability Lab ¬∑ Ann Arbor, MI
              </p>
            </div>
          </div>
        </section>

        <!-- Footer -->
        <footer>
          <p>¬© 2025 Jeremy Zhengqi Huang</p>
        </footer>
      </main>
    </div>

    <!-- JavaScript for Interactivity -->
    <script src="script.js"></script>
  </body>
</html>
